{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACE Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "from contextlib import suppress\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "\n",
    "# Local application imports\n",
    "sys.path.append(\"../src/scripts\")\n",
    "from utilities import (\n",
    "    parse_hdf_data,\n",
    "    merge_dataframes,\n",
    "    sort_columns_except_key,\n",
    "    add_datetime_column,\n",
    ")\n",
    "\n",
    "# Set the warning filter to ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "MISSING_FLAG = -999.900\n",
    "N_SPLITS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data_dir = \"../data/ace/raw\"\n",
    "swics_1hr_dir = f\"{data_dir}/swics_1hr\"\n",
    "swics_2hr_dir = f\"{data_dir}/swics_2hr\"\n",
    "\n",
    "mag_df = parse_hdf_data(f\"{data_dir}/MAG_data_1hr.txt\")\n",
    "swepam_df = parse_hdf_data(f\"{data_dir}/SWEPAM_data_1hr.txt\")\n",
    "epam_df = parse_hdf_data(f\"{data_dir}/EPAM_data_1hr.txt\")\n",
    "\n",
    "swics_dfs = []\n",
    "for dir in [swics_1hr_dir, swics_2hr_dir]:\n",
    "    for file in os.listdir(dir):\n",
    "        swics_dfs.append(parse_hdf_data(f\"{dir}/{file}\"))\n",
    "swics_df = pd.concat(swics_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACE_DATASETS = [mag_df, swepam_df, epam_df, swics_df]\n",
    "ACE_DATASETS_NAMES = [\"MAG\", \"SWEPAM\", \"EPAM\", \"SWICS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype conversion\n",
    "for df in ACE_DATASETS:\n",
    "    df[[\"year\", \"day\", \"hr\", \"min\", \"sec\"]] = df[\n",
    "        [\"year\", \"day\", \"hr\", \"min\", \"sec\"]\n",
    "    ].astype(int)\n",
    "\n",
    "    with suppress(KeyError):\n",
    "        df['Quality'] = df['Quality'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime conversion and drop redundant features\n",
    "for df in ACE_DATASETS:\n",
    "    add_datetime_column(df).drop(\n",
    "        columns=[\"year\", \"day\", \"hr\", \"min\", \"sec\", \"fp_year\", \"fp_doy\"],\n",
    "        inplace=True,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "# swics_df may contain duplicate records to nature of 1.0 and 2.0 data collection\n",
    "swics_df.drop_duplicates(subset=\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, df_name in zip(ACE_DATASETS, ACE_DATASETS_NAMES):\n",
    "    print(f\"Dataframe: {df_name}\")\n",
    "    display(df.info())\n",
    "    display(df.describe())\n",
    "    print(\"\\n\" + (\"-\" * 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain *Good* Quality data\n",
    "\n",
    "Good data is flagged by the researchers with a value of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null flag with np.nan for quality flag analysis\n",
    "for c, (df, df_name) in enumerate(zip(ACE_DATASETS, ACE_DATASETS_NAMES)):\n",
    "    for flag in [-9999.9,-999.9]:\n",
    "        df = df.replace(flag, np.nan)\n",
    "    ACE_DATASETS[c] = df\n",
    "\n",
    "mag_df, swepam_df, epam_df, swics_df = ACE_DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE GOOD CODE\n",
    "# qf_cols = swics_df.filter(regex=\"^qf_\").columns\n",
    "# swics_df = swics_df[(swics_df[qf_cols].isna() | swics_df[qf_cols].eq(0)).any(axis=1)]\n",
    "\n",
    "# for col in swics_df[qf_cols]:\n",
    "#     swics_df = swics_df[(swics_df[col] == 0) | (swics_df[col].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess quality flag provided in the datasets\n",
    "for c, (df, df_name) in enumerate(zip(ACE_DATASETS, ACE_DATASETS_NAMES)):\n",
    "\n",
    "    with suppress(KeyError):  # not all datasets have the quality flag\n",
    "        if df_name != \"SWICS\":\n",
    "            df = df[str(df[\"Quality\"]) == \"0.0\"]\n",
    "            df.drop(columns=[\"Quality\"], inplace=True, axis=1)\n",
    "        else:\n",
    "            qf_cols = swics_df.filter(regex=\"^qf_\").columns\n",
    "            err_cols = swics_df.filter(regex=\"_err$\").columns\n",
    "\n",
    "            # drop rows that have no 0 or nan values due to two instrument versions\n",
    "            df = swics_df[\n",
    "                (swics_df[qf_cols].isna() | swics_df[qf_cols].eq(0)).any(axis=1)\n",
    "            ]\n",
    "\n",
    "            # drop rows that dont have good quality flag 0\n",
    "            for col in df[qf_cols]:\n",
    "                df = df[(df[col] == 0) | (df[col].isna())]\n",
    "\n",
    "            # drop quality columns\n",
    "            df.drop(columns=qf_cols, inplace=True)\n",
    "            df.drop(columns=err_cols, inplace=True)\n",
    "\n",
    "    ACE_DATASETS[c] = df\n",
    "\n",
    "mag_df, swepam_df, epam_df, swics_df = ACE_DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique timestamps\n",
    "mag_dates, swepam_dates, epam_dates, swics_dates = [\n",
    "    df.datetime.unique() for df in ACE_DATASETS\n",
    "]\n",
    "\n",
    "# find the common dates for 1hr interval data\n",
    "common_dates_1hr = reduce(\n",
    "    np.intersect1d, (mag_dates, swepam_dates, epam_dates)\n",
    ")\n",
    "\n",
    "# find the common dates for 2hr interval data\n",
    "common_date_2hr = reduce(\n",
    "    np.intersect1d, (mag_dates, swepam_dates, epam_dates, swics_dates)\n",
    "    )\n",
    "\n",
    "print(len(common_dates_1hr))\n",
    "print(len(common_date_2hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the 1hr to 2 hr interval datasets\n",
    "insitu_df = merge_dataframes(ACE_DATASETS, \"datetime\")\n",
    "df = sort_columns_except_key(insitu_df, \"datetime\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop additional descriptive columns\n",
    "# drop_cols = ['Missing_Proportion', 'Missing_Count']\n",
    "# df.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Missing data has the value of -999.900. Assert that there are no longer missing values due to dropping data labeled as not of good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"datetime\",\n",
    "    \"proton_speed\",\n",
    "    \"proton_density\",\n",
    "    \"proton_temp\",\n",
    "    \"O7to6\",\n",
    "    \"C6to5\",\n",
    "    \"FetoO\",\n",
    "    \"avqFe\",\n",
    "]\n",
    "\n",
    "df_features = df[features].dropna()\n",
    "X = df_features[df_features.select_dtypes(include=\"number\").columns.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation and Min Max Scaler\n",
    "\n",
    "Log transformation is primarily used to reduce skewness in highly skewed data, where some values are much larger than others. This transformation can make the data more \"normal-like\" or symmetric. Applying log transformation before Min-Max scaling can dramatically change the distribution of the data, potentially pulling in large values and spreading out smaller ones. This makes the subsequent Min-Max scaling step distribute the scaled values more evenly across the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the log of zero or negative values is undefined\n",
    "assert (X > 0).all().all(), \"X contains non-positive values\"\n",
    "X = X.apply(np.log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler to the data and transform it\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color=\"black\")\n",
    "plt.xlabel(\"PCA features\")\n",
    "plt.ylabel(\"variance %\")\n",
    "plt.xticks(features)\n",
    "\n",
    "# Save components to a DataFrame\n",
    "PCA_components = pd.DataFrame(pca.transform(X))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_components = np.where(cumulative_variance > 0.95)[0][0] + 1\n",
    "print(\"Number of components to explain 95% Variance: \", num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA that will retain ideal components\n",
    "pca = PCA(n_components=num_components, whiten=True)\n",
    "\n",
    "# Conduct PCA\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Show the new data\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "\n",
    "# The transformed data has been reduced to two dimensions\n",
    "df = pd.DataFrame(\n",
    "    data=X_pca,\n",
    ")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame for better plotting\n",
    "# pc_df = pd.DataFrame(X_pca, columns=[f\"PC{i}\" for i in range(1, num_components + 1)])\n",
    "# sns.pairplot(pc_df)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA Result\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Using Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Kernel PCA with n_components=None to compute all components\n",
    "kpca = KernelPCA(n_components=None, kernel=\"rbf\")\n",
    "kpca.fit(X)\n",
    "\n",
    "# Get eigenvalues\n",
    "eigenvalues = kpca.lambdas_\n",
    "\n",
    "# Plot eigenvalues\n",
    "plt.plot(eigenvalues, \"bo-\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the x-axis represents the index of each component (in descending order of eigenvalue), and the y-axis represents the corresponding eigenvalue. You typically choose the number of components at the point where adding another component doesn't significantly increase the eigenvalue (the \"elbow\" of the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Using Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the size of the encoded representation\n",
    "# encoding_dim = 2  # 2-dimensional encoded representation\n",
    "\n",
    "# # Define the input layer\n",
    "# input_img = Input(shape=(X.shape[1],))\n",
    "\n",
    "# # Define the encoded layer\n",
    "# encoded = Dense(encoding_dim, activation=\"relu\")(input_img)\n",
    "\n",
    "# # Define the decoded layer\n",
    "# decoded = Dense(X.shape[1], activation=\"sigmoid\")(encoded)\n",
    "\n",
    "# # Define the autoencoder model\n",
    "# autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# # Define the encoder model\n",
    "# encoder = Model(input_img, encoded)\n",
    "\n",
    "# # Define the decoder model\n",
    "# encoded_input = Input(shape=(encoding_dim,))\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "# decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# # Compile the autoencoder\n",
    "# autoencoder.compile(optimizer=\"adadelta\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# # Train the autoencoder\n",
    "# autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True)\n",
    "\n",
    "# # Use the encoder to reduce the dimensionality of the data\n",
    "# X_encoded = encoder.predict(X)\n",
    "\n",
    "# print(\"original shape:   \", X.shape)\n",
    "# print(\"transformed shape:\", X_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
