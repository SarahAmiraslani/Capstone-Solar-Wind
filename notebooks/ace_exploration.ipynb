{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACE Exploration\n",
    "\n",
    "ACE (Advanced Composition Explorer) is equipped with nine scientific instruments to make comprehensive and coordinated in situ measurements. These instruments are categorized into two groups: High Resolution Spectrometers and Monitoring Instruments.\n",
    "\n",
    "## High Resolution Spectrometers\n",
    "- **CRIS** - Cosmic Ray Isotope Spectrometer\n",
    "- **SIS** - Solar Isotope Spectrometer\n",
    "- **ULEIS** - Ultra Low Energy Isotope Spectrometer\n",
    "- **SEPICA** - Solar Energetic Particle Ionic Charge Analyzer\n",
    "- **SWICS** - Solar Wind Ion Composition Spectrometer\n",
    "- **SWIMS** - Solar Wind Ion Mass Spectrometer\n",
    "\n",
    "## Monitoring Instruments\n",
    "- **MAG** - Magnetic Field Monitor\n",
    "- **SWEPAM** - Solar Wind Electron, Proton and Alpha Monitor\n",
    "- **EPAM** - Electron, Proton and Alpha Monitor\n",
    "- **SWICS** - Solar Wind Ion Composition Spectrometer\n",
    "\n",
    "All open-source ACE data are formatted using hierarchical data format (HDF). The data are organized by instrument and by time-averaging periods. Each instrument's data are stored in separate HDF data files, and separate HDF files also contain the data from the different averaging periods. For most of the instruments, the data are averaged hourly, daily, and per 27 days (1 Bartels rotation).\n",
    "\n",
    "## About Hierarchical Data Formats\n",
    "Hierarchical Data Formats (HDF) are open source file formats that support large, complex, heterogeneous data. HDF files use a “file directory” like structure that allows you to organize data within the file in many different structured ways, as you might do with files on your computer. HDF files also allow for embedding of metadata making them self-describing.\n",
    "\n",
    "---\n",
    "\n",
    "## Analytical Questions\n",
    "How can we apply novel dimension reduction methods, such as PCA, TSNE, etc., to obtain informative solar wind in-situ data representation in low-dimensional space? How can this low-dimensional representation provide better 2D/3D visualization support than traditional dimension reduction techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "\n",
    "# Third-party imports\n",
    "from contextlib import suppress\n",
    "import warnings\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "# Local application imports\n",
    "sys.path.append(\"../src/scripts\")\n",
    "from utilities import (\n",
    "    parse_hdf_data,\n",
    "    merge_dataframes,\n",
    "    missing_occurrences,\n",
    "    sort_columns_except_key,\n",
    "    visualize_flag,\n",
    "    add_datetime_column,\n",
    ")\n",
    "\n",
    "# Set the warning filter to ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "MISSING_FLAG = -999.900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data_dir = \"../data/ace/raw\"\n",
    "mag_df = parse_hdf_data(f\"{data_dir}/MAG_data_1hr.txt\")\n",
    "swepam_df = parse_hdf_data(f\"{data_dir}/SWEPAM_data_1hr.txt\")\n",
    "epam_df = parse_hdf_data(f\"{data_dir}/EPAM_data_1hr.txt\")\n",
    "swics_df = parse_hdf_data(f\"{data_dir}/SWICS_data_1day.txt\")\n",
    "\n",
    "ACE_DATASETS = [mag_df, swepam_df, epam_df, swics_df]\n",
    "ACE_DATASETS_NAMES = [\"MAG\", \"SWEPAM\", \"EPAM\", \"SWICS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype conversion\n",
    "for df in ACE_DATASETS:\n",
    "    df[[\"year\", \"day\", \"hr\", \"min\", \"sec\"]] = df[\n",
    "        [\"year\", \"day\", \"hr\", \"min\", \"sec\"]\n",
    "    ].astype(int)\n",
    "\n",
    "    with suppress(KeyError):\n",
    "        df['Quality'] = df['Quality'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime conversion and drop redundant features\n",
    "for df in ACE_DATASETS:\n",
    "    add_datetime_column(df).drop(\n",
    "        columns=[\"year\", \"day\", \"hr\", \"min\", \"sec\", \"fp_year\", \"fp_doy\"],\n",
    "        inplace=True,\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain *Good* Quality data\n",
    "\n",
    "Good data is flagged by the researchers with a value of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, (df, df_name) in enumerate(zip(ACE_DATASETS, ACE_DATASETS_NAMES)):\n",
    "\n",
    "    with suppress(KeyError):  # not all datasets have the quality flag\n",
    "        df = df[df[\"Quality\"] == \"0.0\"]\n",
    "        df.drop(columns=[\"Quality\"], inplace=True, axis=1)\n",
    "    ACE_DATASETS[c] = df\n",
    "\n",
    "mag_df, swepam_df, epam_df, swics_df = ACE_DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique timestamps\n",
    "mag_dates, swepam_dates, epam_dates, swics_dates = [\n",
    "    df.datetime.unique() for df in ACE_DATASETS\n",
    "]\n",
    "\n",
    "# find the common dates for 1hr interval data\n",
    "common_dates_1hr = reduce(\n",
    "    np.intersect1d, (mag_dates, swepam_dates, epam_dates)\n",
    ")\n",
    "\n",
    "# find the common dates for 1day interval data\n",
    "common_date_1d = reduce(\n",
    "    np.intersect1d, (mag_dates, swepam_dates, epam_dates, swics_dates)\n",
    "    )\n",
    "\n",
    "print(len(common_dates_1hr))\n",
    "print(len(common_date_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the 1hr interval datasets\n",
    "swics_i = ACE_DATASETS_NAMES.index(\"SWICS\")\n",
    "ACE_DATASETS_1HR = ACE_DATASETS[:swics_i] + ACE_DATASETS[swics_i + 1 :]\n",
    "insitu_1hr_df = merge_dataframes(ACE_DATASETS_1HR, \"datetime\")\n",
    "df = sort_columns_except_key(insitu_1hr_df, \"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Missing data has the value of -999.900. Assert that there are no longer missing values due to dropping data labeled as not of good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = df[df.eq(\"-999.900\").any(axis=1)]\n",
    "missing_occurrences(df, MISSING_FLAG).sort_values(\n",
    "    ascending=False, by=\"Missing_Count\"\n",
    ")\n",
    "visualize_flag(df,df_name)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,df_name in zip(ACE_DATASETS,ACE_DATASETS_NAMES):\n",
    "    print(f\"Dataframe: {df_name}\")\n",
    "    display(df.info())\n",
    "    display(df.describe())\n",
    "    print(\"\\n\"+('-'*20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "- Is there anyway to informatively join these features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = mag_df[mag_df.eq(\"-999.900\").any(axis=1)]\n",
    "mag_df = missing_occurrences(mag_df, MISSING_FLAG).sort_values(ascending=False,by=\"Flag_Count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Organizing Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
