{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACE Exploration\n",
    "\n",
    "ACE (Advanced Composition Explorer) is equipped with nine scientific instruments to make comprehensive and coordinated in situ measurements. These instruments are categorized into two groups: High Resolution Spectrometers and Monitoring Instruments.\n",
    "\n",
    "## High Resolution Spectrometers\n",
    "- **CRIS** - Cosmic Ray Isotope Spectrometer\n",
    "- **SIS** - Solar Isotope Spectrometer\n",
    "- **ULEIS** - Ultra Low Energy Isotope Spectrometer\n",
    "- **SEPICA** - Solar Energetic Particle Ionic Charge Analyzer\n",
    "- **SWICS** - Solar Wind Ion Composition Spectrometer\n",
    "- **SWIMS** - Solar Wind Ion Mass Spectrometer\n",
    "\n",
    "## Monitoring Instruments\n",
    "- **MAG** - Magnetic Field Monitor\n",
    "- **SWEPAM** - Solar Wind Electron, Proton and Alpha Monitor\n",
    "- **EPAM** - Electron, Proton and Alpha Monitor\n",
    "- **SWICS** - Solar Wind Ion Composition Spectrometer\n",
    "\n",
    "All open-source ACE data are formatted using hierarchical data format (HDF). The data are organized by instrument and by time-averaging periods. Each instrument's data are stored in separate HDF data files, and separate HDF files also contain the data from the different averaging periods. For most of the instruments, the data are averaged hourly, daily, and per 27 days (1 Bartels rotation).\n",
    "\n",
    "## About Hierarchical Data Formats\n",
    "Hierarchical Data Formats (HDF) are open source file formats that support large, complex, heterogeneous data. HDF files use a “file directory” like structure that allows you to organize data within the file in many different structured ways, as you might do with files on your computer. HDF files also allow for embedding of metadata making them self-describing.\n",
    "\n",
    "---\n",
    "\n",
    "## Analytical Questions\n",
    "How can we apply novel dimension reduction methods, such as PCA, TSNE, etc., to obtain informative solar wind in-situ data representation in low-dimensional space? How can this low-dimensional representation provide better 2D/3D visualization support than traditional dimension reduction techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "from contextlib import suppress\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "\n",
    "# Local application imports\n",
    "sys.path.append(\"../src/scripts\")\n",
    "from utilities import (\n",
    "    parse_hdf_data,\n",
    "    merge_dataframes,\n",
    "    missing_occurrences,\n",
    "    sort_columns_except_key,\n",
    "    visualize_flag,\n",
    "    add_datetime_column,\n",
    ")\n",
    "\n",
    "# Set the warning filter to ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "MISSING_FLAG = -999.900\n",
    "N_SPLITS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data_dir = \"../data/ace/raw\"\n",
    "swics_1hr_dir = f\"{data_dir}/swics_1hr\"\n",
    "swics_2hr_dir = f\"{data_dir}/swics_2hr\"\n",
    "\n",
    "mag_df = parse_hdf_data(f\"{data_dir}/MAG_data_1hr.txt\")\n",
    "swepam_df = parse_hdf_data(f\"{data_dir}/SWEPAM_data_1hr.txt\")\n",
    "epam_df = parse_hdf_data(f\"{data_dir}/EPAM_data_1hr.txt\")\n",
    "\n",
    "swics_dfs = []\n",
    "for dir in [swics_1hr_dir, swics_2hr_dir]:\n",
    "    for file in os.listdir(dir):\n",
    "        swics_dfs.append(parse_hdf_data(f\"{dir}/{file}\"))\n",
    "swics_df = pd.concat(swics_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACE_DATASETS = [mag_df, swepam_df, epam_df, swics_df]\n",
    "ACE_DATASETS_NAMES = [\"MAG\", \"SWEPAM\", \"EPAM\", \"SWICS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype conversion\n",
    "for df in ACE_DATASETS:\n",
    "    df[[\"year\", \"day\", \"hr\", \"min\", \"sec\"]] = df[\n",
    "        [\"year\", \"day\", \"hr\", \"min\", \"sec\"]\n",
    "    ].astype(int)\n",
    "\n",
    "    with suppress(KeyError):\n",
    "        df['Quality'] = df['Quality'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime conversion and drop redundant features\n",
    "for df in ACE_DATASETS:\n",
    "    add_datetime_column(df).drop(\n",
    "        columns=[\"year\", \"day\", \"hr\", \"min\", \"sec\", \"fp_year\", \"fp_doy\"],\n",
    "        inplace=True,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "# swics_df may contain duplicate records to nature of 1.0 and 2.0 data collection\n",
    "swics_df.drop_duplicates(subset=\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, df_name in zip(ACE_DATASETS, ACE_DATASETS_NAMES):\n",
    "    print(f\"Dataframe: {df_name}\")\n",
    "    display(df.info())\n",
    "    display(df.describe())\n",
    "    print(\"\\n\" + (\"-\" * 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain *Good* Quality data\n",
    "\n",
    "Good data is flagged by the researchers with a value of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, (df, df_name) in enumerate(zip(ACE_DATASETS, ACE_DATASETS_NAMES)):\n",
    "\n",
    "    with suppress(KeyError):  # not all datasets have the quality flag\n",
    "        if df_name != \"SWICS\":\n",
    "            df = df[df[\"Quality\"] == \"0.0\"]\n",
    "            df.drop(columns=[\"Quality\"], inplace=True, axis=1)\n",
    "        else:\n",
    "            qf_cols = swics_df.filter(regex=\"^qf_\").columns\n",
    "            df = swics_df[\n",
    "                (swics_df[qf_cols].isna() | swics_df[qf_cols].eq(0)).any(axis=1)\n",
    "            ]\n",
    "\n",
    "    ACE_DATASETS[c] = df\n",
    "\n",
    "mag_df, swepam_df, epam_df, swics_df = ACE_DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique timestamps\n",
    "mag_dates, swepam_dates, epam_dates, swics_dates = [\n",
    "    df.datetime.unique() for df in ACE_DATASETS\n",
    "]\n",
    "\n",
    "# find the common dates for 1hr interval data\n",
    "common_dates_1hr = reduce(\n",
    "    np.intersect1d, (mag_dates, swepam_dates, epam_dates)\n",
    ")\n",
    "\n",
    "# find the common dates for 2hr interval data\n",
    "common_date_2hr = reduce(\n",
    "    np.intersect1d, (mag_dates, swepam_dates, epam_dates, swics_dates)\n",
    "    )\n",
    "\n",
    "print(len(common_dates_1hr))\n",
    "print(len(common_date_2hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the 1hr to 2 hr interval datasets\n",
    "insitu_df = merge_dataframes(ACE_DATASETS, \"datetime\")\n",
    "df = sort_columns_except_key(insitu_df, \"datetime\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Missing data has the value of -999.900. Assert that there are no longer missing values due to dropping data labeled as not of good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = missing_occurrences(df, MISSING_FLAG).sort_values(\n",
    "    ascending=False, by=\"Missing_Count\"\n",
    ")\n",
    "visualize_flag(df_missing)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation Methods \n",
    "\n",
    "Univariate time series imputation methods: \n",
    "- Mean (median)\n",
    "- Last Observed Carried Forward\n",
    "- Linear Interpolation\n",
    "- Polynomial Interpolation\n",
    "- Kalman Filter\n",
    "- Moving Averge\n",
    "- Random\n",
    "\n",
    "Multivariate Time Series Imputation methods. \n",
    "- K-Nearest Neighbords\n",
    "- Random Forest\n",
    "- Multiple Singular Spectral Analysis\n",
    "- Expectation-Maximization\n",
    "- Multiple Imputation with Chained Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for imputation\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "df = df.replace(-9999.9, np.nan)\n",
    "X_incomplete = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# KNN Imputer with 3 neighbors\n",
    "knn_imputer = KNNImputer(n_neighbors=3, add_indicator=True)\n",
    "knn_imputed_data = knn_imputer.fit_transform(X_incomplete)\n",
    "df_knn_imputed = pd.DataFrame(knn_imputed_data, columns=numeric_df.columns)\n",
    "df_knn_imputed.insert(0, \"datetime\", df[\"datetime\"])\n",
    "\n",
    "# Create the Random Forest imputer\n",
    "rf_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(), random_state=0, max_iter=10\n",
    ")\n",
    "rf_imputed_data = rf_imputer.fit_transform(X_incomplete)\n",
    "df_knn_imputed = pd.DataFrame(rf_imputed_data, columns=numeric_df.columns)\n",
    "df_knn_imputed.insert(0, \"datetime\", df[\"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Random Forest imputer\n",
    "rf_imputer = IterativeImputer(\n",
    "    estimator=RandomForestRegressor(), random_state=0, max_iter=10\n",
    ")\n",
    "\n",
    "# Perform the imputation\n",
    "imputed_data = rf_imputer.fit_transform(X_incomplete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation of all quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: select the subset that needs log transformation\n",
    "df.apply(np.log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color=\"black\")\n",
    "plt.xlabel(\"PCA features\")\n",
    "plt.ylabel(\"variance %\")\n",
    "plt.xticks(features)\n",
    "\n",
    "# Save components to a DataFrame\n",
    "PCA_components = pd.DataFrame(pca.transform(X))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "num_components = np.where(cumulative_variance > 0.95)[0][0] + 1\n",
    "print(\"Number of components to explain 95% Variance: \", num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA that will retain 2 components\n",
    "pca = PCA(n_components=num_components, whiten=True)\n",
    "\n",
    "# Conduct PCA\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Show the new data\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "\n",
    "# The transformed data has been reduced to two dimensions\n",
    "df = pd.DataFrame(\n",
    "    data=X_pca,\n",
    ")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Using Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Kernel PCA with n_components=None to compute all components\n",
    "kpca = KernelPCA(n_components=None, kernel=\"rbf\")\n",
    "kpca.fit(X)\n",
    "\n",
    "# Get eigenvalues\n",
    "eigenvalues = kpca.lambdas_\n",
    "\n",
    "# Plot eigenvalues\n",
    "plt.plot(eigenvalues, \"bo-\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, the x-axis represents the index of each component (in descending order of eigenvalue), and the y-axis represents the corresponding eigenvalue. You typically choose the number of components at the point where adding another component doesn't significantly increase the eigenvalue (the \"elbow\" of the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Using Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the encoded representation\n",
    "encoding_dim = 2  # 2-dimensional encoded representation\n",
    "\n",
    "# Define the input layer\n",
    "input_img = Input(shape=(X.shape[1],))\n",
    "\n",
    "# Define the encoded layer\n",
    "encoded = Dense(encoding_dim, activation=\"relu\")(input_img)\n",
    "\n",
    "# Define the decoded layer\n",
    "decoded = Dense(X.shape[1], activation=\"sigmoid\")(encoded)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# Define the encoder model\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# Define the decoder model\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer=\"adadelta\", loss=\"binary_crossentropy\")\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True)\n",
    "\n",
    "# Use the encoder to reduce the dimensionality of the data\n",
    "X_encoded = encoder.predict(X)\n",
    "\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Organizing Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
