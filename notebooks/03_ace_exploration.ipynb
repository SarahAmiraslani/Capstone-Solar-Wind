{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACE Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Third-party imports\n",
    "from contextlib import suppress\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "# Local application imports\n",
    "sys.path.append(\"../src/scripts\")\n",
    "from utilities import (\n",
    "    parse_hdf_data,\n",
    "    merge_dataframes,\n",
    "    sort_columns_except_key,\n",
    "    add_datetime_column,\n",
    ")\n",
    "\n",
    "# Set the warning filter to ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the ACE data\n",
    "data_dir = \"../data/ace/raw\"\n",
    "swics_1hr_dir = f\"{data_dir}/swics_1hr\"\n",
    "swics_2hr_dir = f\"{data_dir}/swics_2hr\"\n",
    "\n",
    "mag_df = parse_hdf_data(f\"{data_dir}/MAG_data_1hr.txt\")\n",
    "swepam_df = parse_hdf_data(f\"{data_dir}/SWEPAM_data_1hr.txt\")\n",
    "epam_df = parse_hdf_data(f\"{data_dir}/EPAM_data_1hr.txt\")\n",
    "\n",
    "swics_dfs = []\n",
    "for dir in [swics_1hr_dir, swics_2hr_dir]:\n",
    "    for file in os.listdir(dir):\n",
    "        swics_dfs.append(parse_hdf_data(f\"{dir}/{file}\"))\n",
    "swics_df = pd.concat(swics_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "ACE_DATASETS = [mag_df, swepam_df, epam_df, swics_df]\n",
    "ACE_DATASETS_NAMES = [\"MAG\", \"SWEPAM\", \"EPAM\", \"SWICS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype conversion\n",
    "for df in ACE_DATASETS:\n",
    "    df[[\"year\", \"day\", \"hr\", \"min\", \"sec\"]] = df[\n",
    "        [\"year\", \"day\", \"hr\", \"min\", \"sec\"]\n",
    "    ].astype(int)\n",
    "\n",
    "    with suppress(KeyError):\n",
    "        df['Quality'] = df['Quality'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime conversion and drop redundant features\n",
    "for df in ACE_DATASETS:\n",
    "    add_datetime_column(df).drop(\n",
    "        columns=[\"year\", \"day\", \"hr\", \"min\", \"sec\", \"fp_year\", \"fp_doy\"],\n",
    "        inplace=True,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "# swics_df may contain duplicate records to nature of 1.0 and 2.0 data collection\n",
    "swics_df.drop_duplicates(subset=\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, df_name in zip(ACE_DATASETS, ACE_DATASETS_NAMES):\n",
    "    print(f\"Dataframe: {df_name}\")\n",
    "    display(df.info())\n",
    "    display(df.describe())\n",
    "    print(\"\\n\" + (\"-\" * 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain *Good* Quality data\n",
    "\n",
    "Good data is flagged by the researchers with a value of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null flag with np.nan for quality flag analysis\n",
    "for c, (df, df_name) in enumerate(zip(ACE_DATASETS, ACE_DATASETS_NAMES)):\n",
    "    for flag in [-9999.9,-999.9]:\n",
    "        df = df.replace(flag, np.nan)\n",
    "    ACE_DATASETS[c] = df\n",
    "\n",
    "mag_df, swepam_df, epam_df, swics_df = ACE_DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess quality flag provided in the datasets\n",
    "for c, (df, df_name) in enumerate(zip(ACE_DATASETS, ACE_DATASETS_NAMES)):\n",
    "\n",
    "    with suppress(KeyError):  # not all datasets have the quality flag\n",
    "        if df_name != \"SWICS\":\n",
    "            df = df[str(df[\"Quality\"]) == \"0.0\"]\n",
    "            df.drop(columns=[\"Quality\"], inplace=True, axis=1)\n",
    "        else:\n",
    "            qf_cols = swics_df.filter(regex=\"^qf_\").columns\n",
    "            err_cols = swics_df.filter(regex=\"_err$\").columns\n",
    "\n",
    "            # drop rows that have no 0 or nan values due to two instrument versions\n",
    "            df = swics_df[\n",
    "                (swics_df[qf_cols].isna() | swics_df[qf_cols].eq(0)).any(axis=1)\n",
    "            ]\n",
    "\n",
    "            # drop rows that dont have good quality flag 0\n",
    "            for col in df[qf_cols]:\n",
    "                df = df[(df[col] == 0) | (df[col].isna())]\n",
    "\n",
    "            # drop quality columns\n",
    "            df.drop(columns=qf_cols, inplace=True)\n",
    "            df.drop(columns=err_cols, inplace=True)\n",
    "\n",
    "    ACE_DATASETS[c] = df\n",
    "\n",
    "mag_df, swepam_df, epam_df, swics_df = ACE_DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204768\n",
      "132245\n"
     ]
    }
   ],
   "source": [
    "# find unique timestamps\n",
    "mag_dates, swepam_dates, epam_dates, swics_dates = [\n",
    "    df.datetime.unique() for df in ACE_DATASETS\n",
    "]\n",
    "\n",
    "# find the common dates for 1hr interval data\n",
    "common_dates_1hr = reduce(\n",
    "    np.intersect1d, (mag_dates, swepam_dates, epam_dates)\n",
    ")\n",
    "\n",
    "# find the common dates for 2hr interval data\n",
    "common_date_2hr = reduce(\n",
    "    np.intersect1d, (mag_dates, swepam_dates, epam_dates, swics_dates)\n",
    "    )\n",
    "\n",
    "print(len(common_dates_1hr))\n",
    "print(len(common_date_2hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132245, 175)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the 1hr to 2 hr interval datasets\n",
    "insitu_df = merge_dataframes(ACE_DATASETS, \"datetime\")\n",
    "df = sort_columns_except_key(insitu_df, \"datetime\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"../data/ace/preprocessed/insitu_ace.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "Missing data has the value of -999.900. Assert that there are no longer missing values due to dropping data labeled as not of good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"datetime\",\n",
    "    \"proton_speed\",\n",
    "    \"proton_density\",\n",
    "    \"proton_temp\",\n",
    "    \"O7to6\",\n",
    "    \"C6to5\",\n",
    "    \"FetoO\",\n",
    "    \"avqFe\",\n",
    "]\n",
    "\n",
    "df_features = df[features].dropna()\n",
    "X_timestamp = df_features[\"datetime\"].astype(str)\n",
    "X = df_features[df_features.select_dtypes(include=\"number\").columns.tolist()].astype(\n",
    "    float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_count = X[X < 0].count().sum()\n",
    "na_count = X.isna().sum().sum()\n",
    "\n",
    "print(\"Number of non-positive values:\", neg_count)\n",
    "print(\"Number of rows with NaN values:\", na_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log10(X).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation and Min Max Scaler\n",
    "\n",
    "Log transformation is primarily used to reduce skewness in highly skewed data, where some values are much larger than others. This transformation can make the data more \"normal-like\" or symmetric. Applying log transformation before Min-Max scaling can dramatically change the distribution of the data, potentially pulling in large values and spreading out smaller ones. This makes the subsequent Min-Max scaling step distribute the scaled values more evenly across the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the log of zero or negative values is undefined\n",
    "try:\n",
    "    assert (X > 0).all().all(), \"X contains non-positive values\"\n",
    "    X_log = np.log10(X)\n",
    "except AssertionError:\n",
    "    X_log = np.log10(X + 1) # add 1 to avoid log(0)\n",
    "\n",
    "neg_count = X_log[X < 0].count().sum()\n",
    "na_count = X_log.isna().sum().sum()\n",
    "\n",
    "print(\"Number of non-positive values:\", neg_count)\n",
    "print(\"Number of rows with NaN values:\", na_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_timestamp.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_scaled = MinMaxScaler().fit_transform(X).astype(float)\n",
    "df_numeric = pd.DataFrame(X_log_scaled, columns=X.columns)\n",
    "df_datetime = pd.DataFrame(X_timestamp, columns=[\"datetime\"])\n",
    "df_log_scaled = pd.concat([df_datetime, df_numeric], axis=1)\n",
    "df_log_scaled.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_scaled.to_csv(\"../data/ace/preprocessed/insitu_ace_log_scaled.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
